{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b2eaf0af",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-14T14:09:36.651073Z",
     "iopub.status.busy": "2024-04-14T14:09:36.650826Z",
     "iopub.status.idle": "2024-04-14T14:09:37.526862Z",
     "shell.execute_reply": "2024-04-14T14:09:37.526288Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| N/A   69C    P0              32W /  70W |      2MiB / 15360MiB |      0%      Default |\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--\r\n",
      "| N/A   68C    P8              12W /  70W |      5MiB / 15360MiB |      0%      Default |\r\n",
      "--\r\n",
      "| N/A   69C    P8              12W /  70W |      2MiB / 15360MiB |      0%      Default |\r\n"
     ]
    }
   ],
   "source": [
    "! nvidia-smi | grep -B 0 \"W\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "23cd9101",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-14T14:09:37.529901Z",
     "iopub.status.busy": "2024-04-14T14:09:37.529628Z",
     "iopub.status.idle": "2024-04-14T14:09:37.532916Z",
     "shell.execute_reply": "2024-04-14T14:09:37.532453Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "659151a2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-14T14:09:37.535345Z",
     "iopub.status.busy": "2024-04-14T14:09:37.535061Z",
     "iopub.status.idle": "2024-04-14T14:09:41.130721Z",
     "shell.execute_reply": "2024-04-14T14:09:41.130140Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import itertools\n",
    "from torch import nn\n",
    "from trl import DataCollatorForCompletionOnlyLM\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import get_scheduler\n",
    "from transformers.trainer_pt_utils import get_parameter_names\n",
    "from transformers import AutoTokenizer, get_scheduler, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from tqdm.auto import tqdm\n",
    "from peft import LoraConfig, TaskType, get_peft_model, prepare_model_for_kbit_training\n",
    "from datasets import load_dataset\n",
    "import pickle\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a5e2d4b",
   "metadata": {},
   "source": [
    "> set random seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5dd382e0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-14T14:09:41.133852Z",
     "iopub.status.busy": "2024-04-14T14:09:41.133482Z",
     "iopub.status.idle": "2024-04-14T14:09:41.139836Z",
     "shell.execute_reply": "2024-04-14T14:09:41.139377Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f8669335230>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed = 42\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "# np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6432787",
   "metadata": {},
   "source": [
    "> we load model in 4bit, which is actually stored as `torch.uint8` and \n",
    ">\n",
    "> we compute in fp16\n",
    ">\n",
    "> Note that, with `prepare_model_for_kbit_training`, it is actually fp32\n",
    "> for AdaHessian and TRCG to work, fp32 is required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1e6984b5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-14T14:09:41.142318Z",
     "iopub.status.busy": "2024-04-14T14:09:41.142068Z",
     "iopub.status.idle": "2024-04-14T14:09:44.143337Z",
     "shell.execute_reply": "2024-04-14T14:09:44.142753Z"
    }
   },
   "outputs": [],
   "source": [
    "config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=\"float16\",\n",
    ")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"openai-community/gpt2\",\n",
    "                                             torch_dtype=torch.float16,\n",
    "                                             device_map=\"auto\", \n",
    "                                             quantization_config=config)\n",
    "model = prepare_model_for_kbit_training(model, use_gradient_checkpointing=False )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9f127c7",
   "metadata": {},
   "source": [
    "> prep for LoRA model\n",
    ">\n",
    "> check http://localhost:1331/edit/optim/notebooks/README.md\n",
    "> \n",
    "> for currently implemented default `target modules` in `peft`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "608abb3d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-14T14:09:44.146499Z",
     "iopub.status.busy": "2024-04-14T14:09:44.146219Z",
     "iopub.status.idle": "2024-04-14T14:09:44.163386Z",
     "shell.execute_reply": "2024-04-14T14:09:44.162901Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 147,456 || all params: 124,587,264 || trainable%: 0.11835559692522023\n"
     ]
    }
   ],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=4,\n",
    "    lora_alpha=32, # based on paper - https://arxiv.org/abs/2106.09685\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    lora_dropout=0.05\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "318bc216",
   "metadata": {},
   "source": [
    "> at this moment, we need to disable `gradient checkpoint` for\n",
    "> current implementation of AdaHessian and TRCG to work\n",
    "> \n",
    "> Here, for fair comparison, turn off GC for AdamW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b570d99b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-14T14:09:44.165834Z",
     "iopub.status.busy": "2024-04-14T14:09:44.165594Z",
     "iopub.status.idle": "2024-04-14T14:09:44.170302Z",
     "shell.execute_reply": "2024-04-14T14:09:44.169854Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(False,\n",
       " <QuantizationMethod.BITS_AND_BYTES: 'bitsandbytes'>,\n",
       " torch.float32,\n",
       " True,\n",
       " True)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.is_gradient_checkpointing, model.quantization_method, model.dtype,\\\n",
    "model.is_loaded_in_4bit,\\\n",
    "model.config.use_cache"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aee13f9",
   "metadata": {},
   "source": [
    "> set `use_cache` to False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dbea0713",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-14T14:09:44.172804Z",
     "iopub.status.busy": "2024-04-14T14:09:44.172427Z",
     "iopub.status.idle": "2024-04-14T14:09:44.174952Z",
     "shell.execute_reply": "2024-04-14T14:09:44.174500Z"
    }
   },
   "outputs": [],
   "source": [
    "model.config.use_cache = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b37f2f8f",
   "metadata": {},
   "source": [
    "> Start processing data\n",
    "\n",
    "> first is to apply template, we will then use `DataCollatorForCompletionOnlyLM` to mask non-assistant tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c8aa9384",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-14T14:09:44.177518Z",
     "iopub.status.busy": "2024-04-14T14:09:44.177281Z",
     "iopub.status.idle": "2024-04-14T14:09:44.636577Z",
     "shell.execute_reply": "2024-04-14T14:09:44.635987Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2-medium\")\n",
    "tokenizer.add_special_tokens({\"additional_special_tokens\": \\\n",
    "                              [\"<|system|>\",\"<|user|>\",\"<|assistant|>\",\"<|end|>\"]})\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "template_str = \"\"\"\\\n",
    "{% for message in messages %}\\\n",
    "{% if message[\"role\"] == \"user\" %}\\\n",
    "{{ \"<|endoftext|><|user|>\\n\" + message[\"content\"] + \"<|end|>\\n\" }}\\\n",
    "{% elif message[\"role\"] == \"system\" %}\\\n",
    "{{ \"<|system|>\\n\" + message[\"content\"] + \"<|end|>\\n\" }}\\\n",
    "{% elif message[\"role\"] == \"assistant\" %}\\\n",
    "{{ \"<|assistant|>\\n\" + message[\"content\"] + \"<|end|><|endoftext|>\\n\" }}\\\n",
    "{% endif %}\\\n",
    "{% endfor %}\\\n",
    "\"\"\"\n",
    "tokenizer.chat_template = template_str\n",
    "tokenizer.special_tokens_map\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# collator choice\n",
    "collator = DataCollatorForCompletionOnlyLM(\"<|assistant|>\", tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d7b74837",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-14T14:09:44.639749Z",
     "iopub.status.busy": "2024-04-14T14:09:44.639476Z",
     "iopub.status.idle": "2024-04-14T14:09:48.261148Z",
     "shell.execute_reply": "2024-04-14T14:09:48.260550Z"
    }
   },
   "outputs": [],
   "source": [
    "def pair_texts(examples):\n",
    "    # Concatenate all texts.\n",
    "    concatenated_examples = {key: list(itertools.chain(*value)) for key, value in examples.items()}\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "\n",
    "    result = {\n",
    "        k: [t[i : i + 2] for i in range(0, total_length, 2)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    return result\n",
    "\n",
    "def convert_messages(example):\n",
    "    t=[]\n",
    "    for ei in example[\"messages\"]:\n",
    "        if ei[\"role\"]==\"user\":\n",
    "            t.append(\"<|system|>Below is a dialogue between a human user and an AI assistant.<|end|>\\n\")\n",
    "            t.append(\"<|endoftext|><|user|>\" + ei[\"content\"] + \"<|end|>\\n\")\n",
    "        elif ei[\"role\"]==\"assistant\":\n",
    "            t.append(\"<|assistant|>\" + ei[\"content\"] + \"<|end|><|endoftext|>\")\n",
    "    example[\"messages\"]=\"\".join(t)\n",
    "    return example\n",
    "\n",
    "def truncation(example):\n",
    "    \n",
    "    _dialogue = tokenizer(example[\"messages\"],padding=\"max_length\",max_length=128,truncation=True)\n",
    "    \n",
    "    truncate_dialogue = tokenizer.decode(_dialogue[\"input_ids\"])\n",
    "    ending = \"original\"\n",
    "    if \"<|assistant|>\" in truncate_dialogue and \"<|end|><|endoftext|>\" not in truncate_dialogue:\n",
    "        _dialogue[\"input_ids\"][-2:] = [50259, 50256] # add <|end|><|endoftext|> -- cutoff assistant token\n",
    "        ending = \"assistant\"\n",
    "    if \"<|assistant|>\" not in truncate_dialogue:\n",
    "        _dialogue[\"input_ids\"][-2:] = [50259, 198]   # add <|end|>\\n -- no assistant token\n",
    "        ending = \"user\"\n",
    "    \n",
    "    example[\"ending\"]=ending\n",
    "    example[\"messages\"] = truncate_dialogue\n",
    "    return example\n",
    "\n",
    "def token_map(dataset):\n",
    "    dataset = dataset.map(pair_texts, batched=True)\n",
    "    dataset = dataset.map(convert_messages)\n",
    "    dataset = dataset.map(truncation)\n",
    "    dataset = dataset.filter(lambda example: example[\"ending\"]!=\"user\")\n",
    "    dataset = dataset.remove_columns([\"ending\"])\n",
    "    return dataset\n",
    "\n",
    "dataset = load_dataset(\"sablo/oasst2_curated\")\n",
    "dataset = token_map(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5ef4f6e4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-14T14:09:48.264203Z",
     "iopub.status.busy": "2024-04-14T14:09:48.263931Z",
     "iopub.status.idle": "2024-04-14T14:09:48.313528Z",
     "shell.execute_reply": "2024-04-14T14:09:48.313018Z"
    }
   },
   "outputs": [],
   "source": [
    "def tokenize_function(example):\n",
    "    outputs =  tokenizer(example[\"messages\"], \n",
    "                         padding=False, \n",
    "                         truncation=False,\n",
    "                         max_length=128,\n",
    "                         return_overflowing_tokens=False,\n",
    "                         return_length=False)\n",
    "    return {\"input_ids\": outputs[\"input_ids\"], \"attention_mask\": outputs[\"attention_mask\"]}\n",
    "train_dataset = dataset[\"train\"].map(tokenize_function, \n",
    "                                     batched=True, \n",
    "                                     remove_columns=dataset[\"train\"].column_names,\n",
    "                                     num_proc=2,\n",
    "                                     batch_size=1000)\n",
    "test_dataset = dataset[\"test\"].map(tokenize_function, \n",
    "                                   batched=True, \n",
    "                                   remove_columns=dataset[\"test\"].column_names,\n",
    "                                   num_proc=2,\n",
    "                                   batch_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "08394ba9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-14T14:09:48.316217Z",
     "iopub.status.busy": "2024-04-14T14:09:48.315959Z",
     "iopub.status.idle": "2024-04-14T14:09:48.319506Z",
     "shell.execute_reply": "2024-04-14T14:09:48.319022Z"
    }
   },
   "outputs": [],
   "source": [
    "# for training (i.e., fine-tuning)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True, collate_fn=collator, pin_memory=True)\n",
    "# for getting testing perplexity\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=64, shuffle=False, collate_fn=collator, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fce52e1",
   "metadata": {},
   "source": [
    "> AdamW\n",
    "\n",
    "> we need to define parameter group\n",
    "> this is due to the fact that we will\n",
    "> only apply weight decay on non-bias params\n",
    "\n",
    "> we also define algorithm args here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9903af27",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-14T14:09:48.321826Z",
     "iopub.status.busy": "2024-04-14T14:09:48.321583Z",
     "iopub.status.idle": "2024-04-14T14:09:48.329402Z",
     "shell.execute_reply": "2024-04-14T14:09:48.328879Z"
    }
   },
   "outputs": [],
   "source": [
    "decay_parameters = get_parameter_names(model, [nn.LayerNorm])\n",
    "decay_parameters = [name for name in decay_parameters if \"bias\" not in name]\n",
    "\n",
    "learning_rate = 5e-3 # so far the best choice (note: w/o exhaustive search)\n",
    "beta1, beta2 = 0.9, 0.999 # most of time, no need to change\n",
    "epsilon = 1e-8\n",
    "weight_decay = 1e-2 # so far the best w/o exhaustive search\n",
    "\n",
    "optimizer_kwargs = {\n",
    "    \"lr\": learning_rate,\n",
    "    \"betas\": (beta1, beta2),\n",
    "    \"eps\": epsilon,\n",
    "}\n",
    "\n",
    "optimizer_grouped_parameters = [\n",
    "        {\n",
    "            \"params\": [\n",
    "                p for n, p in model.named_parameters() if (n in decay_parameters and p.requires_grad)\n",
    "            ],\n",
    "            \"weight_decay\": weight_decay,\n",
    "        },\n",
    "        {\n",
    "            \"params\": [\n",
    "                p for n, p in model.named_parameters() if (n not in decay_parameters and p.requires_grad)\n",
    "            ],\n",
    "            \"weight_decay\": 0.0,\n",
    "        },\n",
    "    ]\n",
    "\n",
    "optimizer = torch.optim.AdamW(optimizer_grouped_parameters, **optimizer_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b41f0ea1",
   "metadata": {},
   "source": [
    "> define scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "33bd0603",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-14T14:09:48.331698Z",
     "iopub.status.busy": "2024-04-14T14:09:48.331460Z",
     "iopub.status.idle": "2024-04-14T14:09:48.334577Z",
     "shell.execute_reply": "2024-04-14T14:09:48.334122Z"
    }
   },
   "outputs": [],
   "source": [
    "num_epochs = 1\n",
    "num_training_steps = num_epochs * len(train_dataloader)\n",
    "num_warmup_steps = 200 # could be reduced\n",
    "name = \"linear\"\n",
    "scheduler = get_scheduler(name=name,\n",
    "                          optimizer=optimizer,\n",
    "                          num_warmup_steps=num_warmup_steps,\n",
    "                          num_training_steps=num_training_steps\n",
    "                         )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c0d3ac6",
   "metadata": {},
   "source": [
    "> metric function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6fc6b3f5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-14T14:09:48.336819Z",
     "iopub.status.busy": "2024-04-14T14:09:48.336580Z",
     "iopub.status.idle": "2024-04-14T14:09:48.340257Z",
     "shell.execute_reply": "2024-04-14T14:09:48.339795Z"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate(dataloader):\n",
    "    model.eval()\n",
    "    loss = 0\n",
    "    for batch in dataloader:\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**batch)\n",
    "        loss += outputs.loss.item() * batch[\"input_ids\"].shape[0]\n",
    "    loss = loss / len(train_dataloader.dataset)\n",
    "    try:\n",
    "        ppl = np.exp(loss)\n",
    "    except OverflowError:\n",
    "        ppl = float(\"inf\")\n",
    "    return loss, ppl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba9d258f",
   "metadata": {},
   "source": [
    "> training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "be86d649",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-14T14:09:48.342587Z",
     "iopub.status.busy": "2024-04-14T14:09:48.342267Z",
     "iopub.status.idle": "2024-04-14T14:09:48.344795Z",
     "shell.execute_reply": "2024-04-14T14:09:48.344339Z"
    }
   },
   "outputs": [],
   "source": [
    "# log stats\n",
    "# we can measure computational cost by simply logging gradient^ computations\n",
    "#\n",
    "# ^ for second-order methods, in particular, Hessian-free methods, we use gradient and Hessian-vector\n",
    "# product computation as a measure of cost\n",
    "logger = [] # ep, it, loss, ppl, gradient/Hv product, mem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c3f2eee1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-14T14:09:48.347019Z",
     "iopub.status.busy": "2024-04-14T14:09:48.346781Z",
     "iopub.status.idle": "2024-04-14T16:28:49.141922Z",
     "shell.execute_reply": "2024-04-14T16:28:49.141345Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7642f11c767a4a17a4a5be2547c586b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/441 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, iter: 0, tr_loss: 9.05e+01, tr_ppl: 2.10e+39, cost: 0, mem: 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, iter: 10, tr_loss: 8.40e+01, tr_ppl: 1.01e+02, cost: 10, mem: 4.29e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, iter: 20, tr_loss: 1.68e+01, tr_ppl: 2.45e+00, cost: 20, mem: 8.87e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, iter: 30, tr_loss: 6.96e+00, tr_ppl: 1.46e+00, cost: 30, mem: 8.87e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, iter: 40, tr_loss: 6.36e+00, tr_ppl: 1.41e+00, cost: 40, mem: 8.87e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, iter: 50, tr_loss: 5.23e+00, tr_ppl: 1.33e+00, cost: 50, mem: 8.87e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, iter: 60, tr_loss: 4.23e+00, tr_ppl: 1.27e+00, cost: 60, mem: 8.87e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, iter: 70, tr_loss: 3.80e+00, tr_ppl: 1.23e+00, cost: 70, mem: 8.87e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, iter: 80, tr_loss: 3.50e+00, tr_ppl: 1.21e+00, cost: 80, mem: 8.87e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, iter: 90, tr_loss: 3.31e+00, tr_ppl: 1.20e+00, cost: 90, mem: 8.87e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, iter: 100, tr_loss: 3.13e+00, tr_ppl: 1.19e+00, cost: 100, mem: 8.87e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, iter: 110, tr_loss: 2.98e+00, tr_ppl: 1.18e+00, cost: 110, mem: 8.87e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, iter: 120, tr_loss: 2.92e+00, tr_ppl: 1.18e+00, cost: 120, mem: 8.87e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, iter: 130, tr_loss: 2.86e+00, tr_ppl: 1.17e+00, cost: 130, mem: 8.87e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, iter: 140, tr_loss: 2.81e+00, tr_ppl: 1.17e+00, cost: 140, mem: 8.87e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, iter: 150, tr_loss: 2.77e+00, tr_ppl: 1.17e+00, cost: 150, mem: 8.87e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, iter: 160, tr_loss: 2.74e+00, tr_ppl: 1.16e+00, cost: 160, mem: 8.87e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, iter: 170, tr_loss: 2.71e+00, tr_ppl: 1.16e+00, cost: 170, mem: 8.87e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, iter: 180, tr_loss: 2.69e+00, tr_ppl: 1.16e+00, cost: 180, mem: 8.87e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, iter: 190, tr_loss: 2.67e+00, tr_ppl: 1.16e+00, cost: 190, mem: 8.87e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, iter: 200, tr_loss: 2.70e+00, tr_ppl: 1.16e+00, cost: 200, mem: 8.87e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, iter: 210, tr_loss: 2.64e+00, tr_ppl: 1.16e+00, cost: 210, mem: 8.87e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, iter: 220, tr_loss: 2.63e+00, tr_ppl: 1.16e+00, cost: 220, mem: 8.87e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, iter: 230, tr_loss: 2.60e+00, tr_ppl: 1.16e+00, cost: 230, mem: 8.87e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, iter: 240, tr_loss: 2.59e+00, tr_ppl: 1.16e+00, cost: 240, mem: 8.87e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, iter: 250, tr_loss: 2.58e+00, tr_ppl: 1.15e+00, cost: 250, mem: 8.87e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, iter: 260, tr_loss: 2.58e+00, tr_ppl: 1.15e+00, cost: 260, mem: 8.87e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, iter: 270, tr_loss: 2.56e+00, tr_ppl: 1.15e+00, cost: 270, mem: 8.87e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, iter: 280, tr_loss: 2.58e+00, tr_ppl: 1.15e+00, cost: 280, mem: 8.87e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, iter: 290, tr_loss: 2.58e+00, tr_ppl: 1.15e+00, cost: 290, mem: 8.87e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, iter: 300, tr_loss: 2.56e+00, tr_ppl: 1.15e+00, cost: 300, mem: 8.87e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, iter: 310, tr_loss: 2.55e+00, tr_ppl: 1.15e+00, cost: 310, mem: 8.87e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, iter: 320, tr_loss: 2.54e+00, tr_ppl: 1.15e+00, cost: 320, mem: 8.87e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, iter: 330, tr_loss: 2.54e+00, tr_ppl: 1.15e+00, cost: 330, mem: 8.87e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, iter: 340, tr_loss: 2.54e+00, tr_ppl: 1.15e+00, cost: 340, mem: 8.87e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, iter: 350, tr_loss: 2.53e+00, tr_ppl: 1.15e+00, cost: 350, mem: 8.87e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, iter: 360, tr_loss: 2.52e+00, tr_ppl: 1.15e+00, cost: 360, mem: 8.87e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, iter: 370, tr_loss: 2.52e+00, tr_ppl: 1.15e+00, cost: 370, mem: 8.87e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, iter: 380, tr_loss: 2.52e+00, tr_ppl: 1.15e+00, cost: 380, mem: 8.87e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, iter: 390, tr_loss: 2.52e+00, tr_ppl: 1.15e+00, cost: 390, mem: 8.87e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, iter: 400, tr_loss: 2.51e+00, tr_ppl: 1.15e+00, cost: 400, mem: 8.87e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, iter: 410, tr_loss: 2.51e+00, tr_ppl: 1.15e+00, cost: 410, mem: 8.87e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, iter: 420, tr_loss: 2.51e+00, tr_ppl: 1.15e+00, cost: 420, mem: 8.87e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, iter: 430, tr_loss: 2.52e+00, tr_ppl: 1.15e+00, cost: 430, mem: 8.87e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, iter: 440, tr_loss: 2.51e+00, tr_ppl: 1.15e+00, cost: 440, mem: 8.87e+00\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 1\n",
    "num_training_steps = num_epochs * len(train_dataloader)\n",
    "# start memory\n",
    "start_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "# progress bar\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "# accumulative cost\n",
    "cost = 0\n",
    "for epoch in range(num_epochs):\n",
    "    # initialize memory stats\n",
    "    used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3) - start_memory\n",
    "    # compute (training) batch loss and (testing) perplexity\n",
    "    batch_loss, batch_ppl = evaluate(train_dataloader)\n",
    "    # for this demo, we really don't care about validation or testing\n",
    "    # please uncomment the following line if needed\n",
    "    # _, batch_ppl = evaluate(test_dataloader)\n",
    "    # logger\n",
    "    logger.append((epoch, 0, batch_loss, batch_ppl, cost, used_memory))\n",
    "    # print out results\n",
    "    print(f\"epoch: {epoch}, iter: {0}, tr_loss: {batch_loss:.2e}, tr_ppl: {batch_ppl:.2e}, cost: {cost}, mem: {used_memory}\")\n",
    "    model.train()\n",
    "    step_cnt = 0\n",
    "    for it, minibatch in enumerate(train_dataloader, 1):\n",
    "        # forward pass\n",
    "        outputs = model(**minibatch)\n",
    "        loss = outputs.loss\n",
    "        # backward pass\n",
    "        loss.backward()\n",
    "        # optimization step\n",
    "        optimizer.step()\n",
    "        # accumulative cost \n",
    "        cost += 1 # each step requires one gradient computation\n",
    "        scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        # update progress bar\n",
    "        progress_bar.update(1)\n",
    "        \n",
    "        if it % 10 == 0:\n",
    "            # re-compute used memory\n",
    "            used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3) - start_memory\n",
    "            # compute (training) batch loss and (testing) perplexity\n",
    "            batch_loss, batch_ppl = evaluate(train_dataloader)\n",
    "            _, batch_ppl = evaluate(test_dataloader)\n",
    "            # logger\n",
    "            logger.append((epoch, it, batch_loss, batch_ppl, used_memory, cost))\n",
    "            # print out results\n",
    "            print(f\"epoch: {epoch}, iter: {it}, tr_loss: {batch_loss:.2e}, tr_ppl: {batch_ppl:.2e}, cost: {cost}, mem: {used_memory:.2e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c1eeca76",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-14T16:28:49.144886Z",
     "iopub.status.busy": "2024-04-14T16:28:49.144616Z",
     "iopub.status.idle": "2024-04-14T16:28:49.147954Z",
     "shell.execute_reply": "2024-04-14T16:28:49.147509Z"
    }
   },
   "outputs": [],
   "source": [
    "with open('adamw_results.pickle', 'wb') as f:\n",
    "    pickle.dump(logger, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7cc037d8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-14T16:28:49.150150Z",
     "iopub.status.busy": "2024-04-14T16:28:49.149915Z",
     "iopub.status.idle": "2024-04-14T16:28:49.152469Z",
     "shell.execute_reply": "2024-04-14T16:28:49.152030Z"
    }
   },
   "outputs": [],
   "source": [
    "# Exit the program with a success status\n",
    "# also release memory on GPU\n",
    "exit(0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "optimizer_lora",
   "language": "python",
   "name": "optimizerlora"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "24d7586d4ef541f5bb04171bdb67ac60": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "2762e6eda0ed4c26a45531a1c5f6fdf1": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_710b842dad0244b994639917d7fb366e",
       "max": 441.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_d0090aa03da54d10b54e2e8fe271d56b",
       "tabbable": null,
       "tooltip": null,
       "value": 441.0
      }
     },
     "32134252a64847db96a8edd70a4160d5": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "710b842dad0244b994639917d7fb366e": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "71bd74f3636a4f068400353ddb90e156": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_eb99860279154ab6b2c6d12e0d1feff9",
       "placeholder": "​",
       "style": "IPY_MODEL_32134252a64847db96a8edd70a4160d5",
       "tabbable": null,
       "tooltip": null,
       "value": " 441/441 [2:19:00&lt;00:00, 52.42s/it]"
      }
     },
     "7642f11c767a4a17a4a5be2547c586b9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_f9fd36f4ac4d4c5f891b223b473bf716",
        "IPY_MODEL_2762e6eda0ed4c26a45531a1c5f6fdf1",
        "IPY_MODEL_71bd74f3636a4f068400353ddb90e156"
       ],
       "layout": "IPY_MODEL_24d7586d4ef541f5bb04171bdb67ac60",
       "tabbable": null,
       "tooltip": null
      }
     },
     "873d996747524367b68ef8d429d2af66": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "cd26507d229e4fa7b1c271e65b926e44": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "d0090aa03da54d10b54e2e8fe271d56b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "eb99860279154ab6b2c6d12e0d1feff9": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "f9fd36f4ac4d4c5f891b223b473bf716": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_cd26507d229e4fa7b1c271e65b926e44",
       "placeholder": "​",
       "style": "IPY_MODEL_873d996747524367b68ef8d429d2af66",
       "tabbable": null,
       "tooltip": null,
       "value": "100%"
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
