{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b2eaf0af",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-14T14:09:41.816001Z",
     "iopub.status.busy": "2024-04-14T14:09:41.815710Z",
     "iopub.status.idle": "2024-04-14T14:09:42.678781Z",
     "shell.execute_reply": "2024-04-14T14:09:42.678191Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| N/A   70C    P0              32W /  70W |     41MiB / 15360MiB |      0%      Default |\r\n",
      "--\r\n",
      "| N/A   68C    P8              20W /  70W |      5MiB / 15360MiB |      0%      Default |\r\n",
      "--\r\n",
      "| N/A   70C    P0              32W /  70W |      2MiB / 15360MiB |      0%      Default |\r\n"
     ]
    }
   ],
   "source": [
    "! nvidia-smi | grep -B 0 \"W\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "23cd9101",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-14T14:09:42.681902Z",
     "iopub.status.busy": "2024-04-14T14:09:42.681619Z",
     "iopub.status.idle": "2024-04-14T14:09:42.685013Z",
     "shell.execute_reply": "2024-04-14T14:09:42.684496Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"2\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "659151a2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-14T14:09:42.687356Z",
     "iopub.status.busy": "2024-04-14T14:09:42.687114Z",
     "iopub.status.idle": "2024-04-14T14:09:46.643912Z",
     "shell.execute_reply": "2024-04-14T14:09:46.643116Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import itertools\n",
    "from torch import nn\n",
    "from trl import DataCollatorForCompletionOnlyLM\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import get_scheduler\n",
    "from transformers.trainer_pt_utils import get_parameter_names\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, DataCollatorForLanguageModeling, BitsAndBytesConfig\n",
    "from tqdm.auto import tqdm\n",
    "from peft import LoraConfig, TaskType, get_peft_model, prepare_model_for_kbit_training\n",
    "from datasets import load_dataset\n",
    "import sys\n",
    "sys.path.append('../') # please change this path accordingly\n",
    "from optim.trcg import TRCG\n",
    "import pickle\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfd0f0c3",
   "metadata": {},
   "source": [
    "> set random seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "794dbf5d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-14T14:09:46.647454Z",
     "iopub.status.busy": "2024-04-14T14:09:46.646960Z",
     "iopub.status.idle": "2024-04-14T14:09:46.650942Z",
     "shell.execute_reply": "2024-04-14T14:09:46.650348Z"
    }
   },
   "outputs": [],
   "source": [
    "seed = 42\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fc198da",
   "metadata": {},
   "source": [
    "> we load model in 4bit, which is actually stored as `torch.uint8` and \n",
    ">\n",
    "> we compute in fp16\n",
    ">\n",
    "> Note that, with `prepare_model_for_kbit_training`, it is actually fp32\n",
    "> for AdaHessian and TRCG to work, fp32 is required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1e6984b5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-14T14:09:46.653575Z",
     "iopub.status.busy": "2024-04-14T14:09:46.653267Z",
     "iopub.status.idle": "2024-04-14T14:09:48.911743Z",
     "shell.execute_reply": "2024-04-14T14:09:48.910721Z"
    }
   },
   "outputs": [],
   "source": [
    "config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=\"float16\",\n",
    ")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"openai-community/gpt2\",\n",
    "                                             torch_dtype=torch.float16,\n",
    "                                             device_map=\"auto\", \n",
    "                                             quantization_config=config)\n",
    "model = prepare_model_for_kbit_training(model, use_gradient_checkpointing=False )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9f127c7",
   "metadata": {},
   "source": [
    "> prep for LoRA model\n",
    ">\n",
    "> check http://localhost:1331/edit/optim/notebooks/README.md\n",
    "> \n",
    "> for currently implemented default `target modules` in `peft`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "608abb3d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-14T14:09:48.915235Z",
     "iopub.status.busy": "2024-04-14T14:09:48.914810Z",
     "iopub.status.idle": "2024-04-14T14:09:48.934552Z",
     "shell.execute_reply": "2024-04-14T14:09:48.933844Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 147,456 || all params: 124,587,264 || trainable%: 0.11835559692522023\n"
     ]
    }
   ],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=4, \n",
    "    lora_alpha=32, # based on paper - https://arxiv.org/abs/2106.09685\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    lora_dropout=0.05\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2081d64f",
   "metadata": {},
   "source": [
    "> at this moment, we need to disable `gradient checkpoint` for\n",
    "> current implementation of AdaHessian and TRCG to work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b570d99b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-14T14:09:48.937446Z",
     "iopub.status.busy": "2024-04-14T14:09:48.936981Z",
     "iopub.status.idle": "2024-04-14T14:09:48.945553Z",
     "shell.execute_reply": "2024-04-14T14:09:48.944931Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(False,\n",
       " <QuantizationMethod.BITS_AND_BYTES: 'bitsandbytes'>,\n",
       " torch.float32,\n",
       " True,\n",
       " True)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.is_gradient_checkpointing, model.quantization_method, model.dtype,\\\n",
    "model.is_loaded_in_4bit,\\\n",
    "model.config.use_cache"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cecbb0b",
   "metadata": {},
   "source": [
    "> set `use_cache` to False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3ef9474e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-14T14:09:48.948496Z",
     "iopub.status.busy": "2024-04-14T14:09:48.948148Z",
     "iopub.status.idle": "2024-04-14T14:09:48.951503Z",
     "shell.execute_reply": "2024-04-14T14:09:48.950907Z"
    }
   },
   "outputs": [],
   "source": [
    "model.config.use_cache = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37bb661f",
   "metadata": {},
   "source": [
    "> Start processing data\n",
    "\n",
    "> first is to apply template, we will then use `DataCollatorForCompletionOnlyLM` to mask non-assistant tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c8aa9384",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-14T14:09:48.954218Z",
     "iopub.status.busy": "2024-04-14T14:09:48.953868Z",
     "iopub.status.idle": "2024-04-14T14:09:49.407327Z",
     "shell.execute_reply": "2024-04-14T14:09:49.406742Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2-medium\")\n",
    "tokenizer.add_special_tokens({\"additional_special_tokens\": \\\n",
    "                              [\"<|system|>\",\"<|user|>\",\"<|assistant|>\",\"<|end|>\"]})\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "template_str = \"\"\"\\\n",
    "{% for message in messages %}\\\n",
    "{% if message[\"role\"] == \"user\" %}\\\n",
    "{{ \"<|endoftext|><|user|>\\n\" + message[\"content\"] + \"<|end|>\\n\" }}\\\n",
    "{% elif message[\"role\"] == \"system\" %}\\\n",
    "{{ \"<|system|>\\n\" + message[\"content\"] + \"<|end|>\\n\" }}\\\n",
    "{% elif message[\"role\"] == \"assistant\" %}\\\n",
    "{{ \"<|assistant|>\\n\" + message[\"content\"] + \"<|end|><|endoftext|>\\n\" }}\\\n",
    "{% endif %}\\\n",
    "{% endfor %}\\\n",
    "\"\"\"\n",
    "tokenizer.chat_template = template_str\n",
    "tokenizer.special_tokens_map\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# collator choice\n",
    "collator = DataCollatorForCompletionOnlyLM(\"<|assistant|>\", tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d7b74837",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-14T14:09:49.410485Z",
     "iopub.status.busy": "2024-04-14T14:09:49.410085Z",
     "iopub.status.idle": "2024-04-14T14:09:52.460290Z",
     "shell.execute_reply": "2024-04-14T14:09:52.459307Z"
    }
   },
   "outputs": [],
   "source": [
    "def pair_texts(examples):\n",
    "    # Concatenate all texts.\n",
    "    concatenated_examples = {key: list(itertools.chain(*value)) for key, value in examples.items()}\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "\n",
    "    result = {\n",
    "        k: [t[i : i + 2] for i in range(0, total_length, 2)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    return result\n",
    "\n",
    "def convert_messages(example):\n",
    "    t=[]\n",
    "    for ei in example[\"messages\"]:\n",
    "        if ei[\"role\"]==\"user\":\n",
    "            t.append(\"<|system|>Below is a dialogue between a human user and an AI assistant.<|end|>\\n\")\n",
    "            t.append(\"<|endoftext|><|user|>\" + ei[\"content\"] + \"<|end|>\\n\")\n",
    "        elif ei[\"role\"]==\"assistant\":\n",
    "            t.append(\"<|assistant|>\" + ei[\"content\"] + \"<|end|><|endoftext|>\")\n",
    "    example[\"messages\"]=\"\".join(t)\n",
    "    return example\n",
    "\n",
    "def truncation(example):\n",
    "    \n",
    "    _dialogue = tokenizer(example[\"messages\"],padding=\"max_length\",max_length=128,truncation=True)\n",
    "    \n",
    "    truncate_dialogue = tokenizer.decode(_dialogue[\"input_ids\"])\n",
    "    ending = \"original\"\n",
    "    if \"<|assistant|>\" in truncate_dialogue and \"<|end|><|endoftext|>\" not in truncate_dialogue:\n",
    "        _dialogue[\"input_ids\"][-2:] = [50259, 50256] # add <|end|><|endoftext|> -- cutoff assistant token\n",
    "        ending = \"assistant\"\n",
    "    if \"<|assistant|>\" not in truncate_dialogue:\n",
    "        _dialogue[\"input_ids\"][-2:] = [50259, 198]   # add <|end|>\\n -- no assistant token\n",
    "        ending = \"user\"\n",
    "    \n",
    "    example[\"ending\"]=ending\n",
    "    example[\"messages\"] = truncate_dialogue\n",
    "    return example\n",
    "\n",
    "def token_map(dataset):\n",
    "    dataset = dataset.map(pair_texts, batched=True)\n",
    "    dataset = dataset.map(convert_messages)\n",
    "    dataset = dataset.map(truncation)\n",
    "    dataset = dataset.filter(lambda example: example[\"ending\"]!=\"user\")\n",
    "    dataset = dataset.remove_columns([\"ending\"])\n",
    "    return dataset\n",
    "\n",
    "dataset = load_dataset(\"sablo/oasst2_curated\")\n",
    "dataset = token_map(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5ef4f6e4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-14T14:09:52.463572Z",
     "iopub.status.busy": "2024-04-14T14:09:52.463215Z",
     "iopub.status.idle": "2024-04-14T14:09:52.512994Z",
     "shell.execute_reply": "2024-04-14T14:09:52.512361Z"
    }
   },
   "outputs": [],
   "source": [
    "def tokenize_function(example):\n",
    "    outputs =  tokenizer(example[\"messages\"], \n",
    "                         padding=False, \n",
    "                         truncation=False,\n",
    "                         max_length=128,\n",
    "                         return_overflowing_tokens=False,\n",
    "                         return_length=False)\n",
    "    return {\"input_ids\": outputs[\"input_ids\"], \"attention_mask\": outputs[\"attention_mask\"]}\n",
    "train_dataset = dataset[\"train\"].map(tokenize_function, \n",
    "                                     batched=True, \n",
    "                                     remove_columns=dataset[\"train\"].column_names,\n",
    "                                     num_proc=2,\n",
    "                                     batch_size=1000)\n",
    "test_dataset = dataset[\"test\"].map(tokenize_function, \n",
    "                                   batched=True, \n",
    "                                   remove_columns=dataset[\"test\"].column_names,\n",
    "                                   num_proc=2,\n",
    "                                   batch_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "08394ba9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-14T14:09:52.516017Z",
     "iopub.status.busy": "2024-04-14T14:09:52.515606Z",
     "iopub.status.idle": "2024-04-14T14:09:52.520260Z",
     "shell.execute_reply": "2024-04-14T14:09:52.519581Z"
    }
   },
   "outputs": [],
   "source": [
    "# for training (i.e., fine-tuning)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True, collate_fn=collator, pin_memory=True)\n",
    "# for getting testing perplexity\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=64, shuffle=False, collate_fn=collator, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e11e7e73",
   "metadata": {},
   "source": [
    "> metric function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "205dd815",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-14T14:09:52.523301Z",
     "iopub.status.busy": "2024-04-14T14:09:52.522799Z",
     "iopub.status.idle": "2024-04-14T14:09:52.527325Z",
     "shell.execute_reply": "2024-04-14T14:09:52.526639Z"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate(dataloader):\n",
    "    model.eval()\n",
    "    loss = 0\n",
    "    for batch in dataloader:\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**batch)\n",
    "        loss += outputs.loss.item() * batch[\"input_ids\"].shape[0]\n",
    "    loss = loss / len(train_dataloader.dataset)\n",
    "    try:\n",
    "        ppl = np.exp(loss)\n",
    "    except OverflowError:\n",
    "        ppl = float(\"inf\")\n",
    "    return loss, ppl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1db66b33",
   "metadata": {},
   "source": [
    "> define optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5b3ef635",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-14T14:09:52.530015Z",
     "iopub.status.busy": "2024-04-14T14:09:52.529648Z",
     "iopub.status.idle": "2024-04-14T14:09:52.538617Z",
     "shell.execute_reply": "2024-04-14T14:09:52.537848Z"
    }
   },
   "outputs": [],
   "source": [
    "# the hyperparameters are default setup\n",
    "# please see /optim/trcg.py for details\n",
    "# it is almost always the case that there \n",
    "# is no need to fine-tune any of the hyper-parameters\n",
    "# ZERO fine-tunining effort, yay!\n",
    "optimizer = TRCG(model, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ff0e709",
   "metadata": {},
   "source": [
    "> training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ed752d0a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-14T14:09:52.541712Z",
     "iopub.status.busy": "2024-04-14T14:09:52.541235Z",
     "iopub.status.idle": "2024-04-14T14:09:52.544511Z",
     "shell.execute_reply": "2024-04-14T14:09:52.543848Z"
    }
   },
   "outputs": [],
   "source": [
    "# log stats\n",
    "# we can measure computational cost by simply logging gradient^ computations\n",
    "#\n",
    "# ^ for second-order methods, in particular, Hessian-free methods, we use gradient and Hessian-vector\n",
    "# product computation as a measure of cost\n",
    "logger = [] # ep, it, loss, ppl, gradient/Hv product, mem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "87a5030d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-14T14:09:52.547178Z",
     "iopub.status.busy": "2024-04-14T14:09:52.546700Z",
     "iopub.status.idle": "2024-04-14T16:27:40.171792Z",
     "shell.execute_reply": "2024-04-14T16:27:40.171250Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbdb3dbe4e3b49909996bfa55cc4875c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/441 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, tr_loss: 9.05e+01, tr_ppl: 2.10e+39, cost: 0, mem: 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, iter: 10, tr_loss: 1.13e+01, tr_ppl: 7.88e+04, cost: 33.0, mem: 1.29e+01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, iter: 20, tr_loss: 5.31e+00, tr_ppl: 2.02e+02, cost: 68.0, mem: 1.29e+01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, iter: 30, tr_loss: 4.63e+00, tr_ppl: 1.03e+02, cost: 101.0, mem: 1.29e+01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, iter: 40, tr_loss: 4.64e+00, tr_ppl: 1.03e+02, cost: 134.0, mem: 1.29e+01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, iter: 50, tr_loss: 3.82e+00, tr_ppl: 4.55e+01, cost: 167.0, mem: 1.29e+01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, iter: 60, tr_loss: 3.65e+00, tr_ppl: 3.84e+01, cost: 200.0, mem: 1.29e+01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, iter: 70, tr_loss: 3.29e+00, tr_ppl: 2.68e+01, cost: 234.0, mem: 1.29e+01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, iter: 80, tr_loss: 3.10e+00, tr_ppl: 2.22e+01, cost: 269.0, mem: 1.29e+01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, iter: 90, tr_loss: 3.16e+00, tr_ppl: 2.36e+01, cost: 301.0, mem: 1.29e+01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, iter: 100, tr_loss: 2.93e+00, tr_ppl: 1.88e+01, cost: 334.0, mem: 1.29e+01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, iter: 110, tr_loss: 3.14e+00, tr_ppl: 2.30e+01, cost: 371.0, mem: 1.29e+01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, iter: 120, tr_loss: 2.81e+00, tr_ppl: 1.66e+01, cost: 407.0, mem: 1.29e+01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, iter: 130, tr_loss: 2.83e+00, tr_ppl: 1.69e+01, cost: 444.0, mem: 1.29e+01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, iter: 140, tr_loss: 2.76e+00, tr_ppl: 1.58e+01, cost: 480.0, mem: 1.29e+01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, iter: 150, tr_loss: 2.79e+00, tr_ppl: 1.62e+01, cost: 518.0, mem: 1.29e+01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, iter: 160, tr_loss: 2.75e+00, tr_ppl: 1.57e+01, cost: 557.0, mem: 1.29e+01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, iter: 170, tr_loss: 2.83e+00, tr_ppl: 1.69e+01, cost: 591.0, mem: 1.29e+01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, iter: 180, tr_loss: 2.71e+00, tr_ppl: 1.51e+01, cost: 633.0, mem: 1.29e+01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, iter: 190, tr_loss: 2.75e+00, tr_ppl: 1.57e+01, cost: 674.0, mem: 1.29e+01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, iter: 200, tr_loss: 2.66e+00, tr_ppl: 1.43e+01, cost: 707.0, mem: 1.29e+01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, iter: 210, tr_loss: 2.69e+00, tr_ppl: 1.47e+01, cost: 744.0, mem: 1.29e+01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, iter: 220, tr_loss: 2.66e+00, tr_ppl: 1.43e+01, cost: 780.0, mem: 1.29e+01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, iter: 230, tr_loss: 2.63e+00, tr_ppl: 1.39e+01, cost: 821.0, mem: 1.29e+01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, iter: 240, tr_loss: 2.64e+00, tr_ppl: 1.40e+01, cost: 857.0, mem: 1.29e+01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, iter: 250, tr_loss: 2.62e+00, tr_ppl: 1.37e+01, cost: 893.0, mem: 1.29e+01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, iter: 260, tr_loss: 2.62e+00, tr_ppl: 1.38e+01, cost: 932.0, mem: 1.29e+01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, iter: 270, tr_loss: 2.61e+00, tr_ppl: 1.36e+01, cost: 967.0, mem: 1.29e+01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, iter: 280, tr_loss: 2.74e+00, tr_ppl: 1.55e+01, cost: 1003.0, mem: 1.29e+01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, iter: 290, tr_loss: 2.99e+00, tr_ppl: 2.00e+01, cost: 1044.0, mem: 1.29e+01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, iter: 300, tr_loss: 2.67e+00, tr_ppl: 1.44e+01, cost: 1081.0, mem: 1.29e+01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, iter: 310, tr_loss: 2.68e+00, tr_ppl: 1.46e+01, cost: 1119.0, mem: 1.29e+01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, iter: 320, tr_loss: 2.60e+00, tr_ppl: 1.34e+01, cost: 1158.0, mem: 1.29e+01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, iter: 330, tr_loss: 2.60e+00, tr_ppl: 1.35e+01, cost: 1197.0, mem: 1.29e+01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, iter: 340, tr_loss: 2.61e+00, tr_ppl: 1.36e+01, cost: 1234.0, mem: 1.29e+01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, iter: 350, tr_loss: 2.58e+00, tr_ppl: 1.32e+01, cost: 1268.0, mem: 1.29e+01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, iter: 360, tr_loss: 2.71e+00, tr_ppl: 1.50e+01, cost: 1308.0, mem: 1.29e+01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, iter: 370, tr_loss: 2.65e+00, tr_ppl: 1.42e+01, cost: 1353.0, mem: 1.29e+01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, iter: 380, tr_loss: 2.58e+00, tr_ppl: 1.32e+01, cost: 1389.0, mem: 1.29e+01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, iter: 390, tr_loss: 2.58e+00, tr_ppl: 1.32e+01, cost: 1423.0, mem: 1.29e+01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, iter: 400, tr_loss: 2.59e+00, tr_ppl: 1.33e+01, cost: 1462.0, mem: 1.29e+01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, iter: 410, tr_loss: 2.64e+00, tr_ppl: 1.40e+01, cost: 1502.0, mem: 1.29e+01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, iter: 420, tr_loss: 2.61e+00, tr_ppl: 1.36e+01, cost: 1538.0, mem: 1.29e+01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, iter: 430, tr_loss: 2.58e+00, tr_ppl: 1.33e+01, cost: 1573.0, mem: 1.29e+01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, iter: 440, tr_loss: 2.56e+00, tr_ppl: 1.30e+01, cost: 1608.0, mem: 1.29e+01\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 1\n",
    "num_training_steps = num_epochs * len(train_dataloader)\n",
    "# start memory\n",
    "start_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "# progress bar\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "# accumulative cost\n",
    "cost = 0\n",
    "# training loop\n",
    "for epoch in range(num_epochs):\n",
    "    # initialize memory stats\n",
    "    used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3) - start_memory\n",
    "    # compute (training) batch loss and (testing) perplexity\n",
    "    batch_loss, batch_ppl = evaluate(train_dataloader)\n",
    "    # _, batch_ppl = evaluate(test_dataloader)\n",
    "    # logger\n",
    "    logger.append((epoch, 0, batch_loss, batch_ppl, cost, used_memory))\n",
    "    # print out results\n",
    "    print(f\"epoch: {epoch}, tr_loss: {batch_loss:.2e}, tr_ppl: {batch_ppl:.2e}, cost: {cost}, mem: {used_memory}\")\n",
    "    model.train()\n",
    "    step_cnt = 0\n",
    "    for it, minibatch in enumerate(train_dataloader, 1):\n",
    "        # forward pass\n",
    "        outputs = model(**minibatch)\n",
    "        loss = outputs.loss.item()\n",
    "        # subject to current implementation\n",
    "        # need to explicitly define lora param\n",
    "        lora_param = [w for w in model.parameters() if w.requires_grad]\n",
    "        # explictly compute the gradient with `create_graph = True`\n",
    "        # note: it is probably better to use `loss.backward(create_graph=True)`\n",
    "        #       current implementation requires this form\n",
    "        V = torch.autograd.grad(outputs.loss, lora_param, create_graph=True)\n",
    "        # compute square norm the gradient, to monitor progress\n",
    "        # commented out \n",
    "        # V_norm = np.sum([torch.sum(vi.data**2).item() for vi in V])**0.5\n",
    "        \n",
    "        # temporary fix of constantly shrinking radius\n",
    "        optimizer.radius *= 2.0\n",
    "        # optimization step\n",
    "        _, _, cg_cost = optimizer.step(minibatch, loss, V)\n",
    "        # accumulative cost\n",
    "        cost += cg_cost + 1 # cg_cost is # of gradient and Hv computations\n",
    "        # update progress bar\n",
    "        progress_bar.update(1)\n",
    "        \n",
    "        if it % 10 == 0:\n",
    "            # re-compute used memory\n",
    "            used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3) - start_memory\n",
    "            # compute (training) batch loss and (testing) perplexity\n",
    "            batch_loss, batch_ppl = evaluate(train_dataloader)\n",
    "            # _, batch_ppl = evaluate(test_dataloader)\n",
    "            # logger\n",
    "            logger.append((epoch, it, batch_loss, batch_ppl, cost, used_memory))\n",
    "            # print out results\n",
    "            print(f\"epoch: {epoch}, iter: {it}, tr_loss: {batch_loss:.2e}, tr_ppl: {batch_ppl:.2e}, cost: {cost}, mem: {used_memory:.2e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "284a574c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-14T16:27:40.174617Z",
     "iopub.status.busy": "2024-04-14T16:27:40.174282Z",
     "iopub.status.idle": "2024-04-14T16:27:40.177998Z",
     "shell.execute_reply": "2024-04-14T16:27:40.177541Z"
    }
   },
   "outputs": [],
   "source": [
    "with open('trcg_results.pickle', 'wb') as f:\n",
    "    pickle.dump(logger, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "88f9321b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-14T16:27:40.180494Z",
     "iopub.status.busy": "2024-04-14T16:27:40.179988Z",
     "iopub.status.idle": "2024-04-14T16:27:40.182731Z",
     "shell.execute_reply": "2024-04-14T16:27:40.182284Z"
    }
   },
   "outputs": [],
   "source": [
    "# Exit the program with a success status\n",
    "# also release memory on GPU\n",
    "exit(0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "optimizer_lora",
   "language": "python",
   "name": "optimizerlora"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "483d8ca19a584d87b7c7ffbba2f4d167": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "98655f093780403a9f86aff9eba783b8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_f2adaabee4144dcaa106da3ee80371f8",
       "max": 441.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_e32c968846fb4bbaa0e60aa4be5af7e0",
       "tabbable": null,
       "tooltip": null,
       "value": 441.0
      }
     },
     "9c42068cc2b64c21a39404125ecd4873": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "a6357e8363f648eab7c39a176e09f97b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_b0ad5824fde243028ff98622e36ea6f0",
       "placeholder": "​",
       "style": "IPY_MODEL_ca219c350bf54bea8a067a56b7082ce5",
       "tabbable": null,
       "tooltip": null,
       "value": " 441/441 [2:17:47&lt;00:00, 48.34s/it]"
      }
     },
     "b0ad5824fde243028ff98622e36ea6f0": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "ca219c350bf54bea8a067a56b7082ce5": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "dbdb3dbe4e3b49909996bfa55cc4875c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_e9079c43efcc40618f31d76c8e82af40",
        "IPY_MODEL_98655f093780403a9f86aff9eba783b8",
        "IPY_MODEL_a6357e8363f648eab7c39a176e09f97b"
       ],
       "layout": "IPY_MODEL_483d8ca19a584d87b7c7ffbba2f4d167",
       "tabbable": null,
       "tooltip": null
      }
     },
     "e32c968846fb4bbaa0e60aa4be5af7e0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "e9079c43efcc40618f31d76c8e82af40": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_9c42068cc2b64c21a39404125ecd4873",
       "placeholder": "​",
       "style": "IPY_MODEL_fbc39f7ce9494417a74c6511e4cf073e",
       "tabbable": null,
       "tooltip": null,
       "value": "100%"
      }
     },
     "f2adaabee4144dcaa106da3ee80371f8": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "fbc39f7ce9494417a74c6511e4cf073e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
