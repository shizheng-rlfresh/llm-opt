{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b2eaf0af",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-14T14:09:45.047073Z",
     "iopub.status.busy": "2024-04-14T14:09:45.046795Z",
     "iopub.status.idle": "2024-04-14T14:09:45.597701Z",
     "shell.execute_reply": "2024-04-14T14:09:45.597077Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| N/A   70C    P0              32W /  70W |    485MiB / 15360MiB |      0%      Default |\r\n",
      "--\r\n",
      "| N/A   68C    P8              11W /  70W |      5MiB / 15360MiB |      0%      Default |\r\n",
      "--\r\n",
      "| N/A   70C    P8              32W /  70W |      2MiB / 15360MiB |      0%      Default |\r\n"
     ]
    }
   ],
   "source": [
    "! nvidia-smi | grep -B 0 \"W\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "23cd9101",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-14T14:09:45.600834Z",
     "iopub.status.busy": "2024-04-14T14:09:45.600561Z",
     "iopub.status.idle": "2024-04-14T14:09:45.603803Z",
     "shell.execute_reply": "2024-04-14T14:09:45.603350Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "659151a2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-14T14:09:45.606210Z",
     "iopub.status.busy": "2024-04-14T14:09:45.605887Z",
     "iopub.status.idle": "2024-04-14T14:09:49.259375Z",
     "shell.execute_reply": "2024-04-14T14:09:49.258512Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import itertools\n",
    "from torch import nn\n",
    "from trl import DataCollatorForCompletionOnlyLM\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers.trainer_pt_utils import get_parameter_names\n",
    "from transformers import AutoTokenizer,AutoModelForCausalLM,DataCollatorForLanguageModeling, BitsAndBytesConfig\n",
    "from tqdm.auto import tqdm\n",
    "from accelerate import Accelerator\n",
    "from peft import LoraConfig, TaskType, get_peft_model, prepare_model_for_kbit_training\n",
    "from datasets import load_dataset\n",
    "import torch_optimizer as optim # `torch_optimizer` package for AdaHessian\n",
    "import pickle\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda35ee8",
   "metadata": {},
   "source": [
    "> set random seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "03c63e30",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-14T14:09:49.263503Z",
     "iopub.status.busy": "2024-04-14T14:09:49.263009Z",
     "iopub.status.idle": "2024-04-14T14:09:49.267040Z",
     "shell.execute_reply": "2024-04-14T14:09:49.266426Z"
    }
   },
   "outputs": [],
   "source": [
    "seed = 42\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fc198da",
   "metadata": {},
   "source": [
    "> we load model in 4bit, which is actually stored as `torch.uint8` and \n",
    ">\n",
    "> we compute in fp16\n",
    ">\n",
    "> Note that, with `prepare_model_for_kbit_training`, it is actually fp32\n",
    "> for AdaHessian and TRCG to work, fp32 is required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1e6984b5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-14T14:09:49.270304Z",
     "iopub.status.busy": "2024-04-14T14:09:49.269959Z",
     "iopub.status.idle": "2024-04-14T14:09:51.178802Z",
     "shell.execute_reply": "2024-04-14T14:09:51.177752Z"
    }
   },
   "outputs": [],
   "source": [
    "config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=\"float16\",\n",
    ")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"openai-community/gpt2\",\n",
    "                                             torch_dtype=torch.float16,\n",
    "                                             device_map=\"auto\", \n",
    "                                             quantization_config=config)\n",
    "model = prepare_model_for_kbit_training(model, use_gradient_checkpointing=False )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9f127c7",
   "metadata": {},
   "source": [
    "> prep for LoRA model\n",
    ">\n",
    "> check http://localhost:1331/edit/optim/notebooks/README.md\n",
    "> \n",
    "> for currently implemented default `target modules` in `peft`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "608abb3d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-14T14:09:51.182184Z",
     "iopub.status.busy": "2024-04-14T14:09:51.181852Z",
     "iopub.status.idle": "2024-04-14T14:09:51.200325Z",
     "shell.execute_reply": "2024-04-14T14:09:51.199626Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 147,456 || all params: 124,587,264 || trainable%: 0.11835559692522023\n"
     ]
    }
   ],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=4, \n",
    "    lora_alpha=32, # based on paper - https://arxiv.org/abs/2106.09685\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    lora_dropout=0.05\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bdb414a",
   "metadata": {},
   "source": [
    "> at this moment, we need to disable `gradient checkpoint` for\n",
    "> current implementation of AdaHessian and TRCG to work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b570d99b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-14T14:09:51.203065Z",
     "iopub.status.busy": "2024-04-14T14:09:51.202632Z",
     "iopub.status.idle": "2024-04-14T14:09:51.210444Z",
     "shell.execute_reply": "2024-04-14T14:09:51.209819Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(False,\n",
       " <QuantizationMethod.BITS_AND_BYTES: 'bitsandbytes'>,\n",
       " torch.float32,\n",
       " True,\n",
       " True)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.is_gradient_checkpointing, model.quantization_method, model.dtype,\\\n",
    "model.is_loaded_in_4bit,\\\n",
    "model.config.use_cache"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7516c59",
   "metadata": {},
   "source": [
    "> set `use_cache` to False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "23581087",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-14T14:09:51.213109Z",
     "iopub.status.busy": "2024-04-14T14:09:51.212790Z",
     "iopub.status.idle": "2024-04-14T14:09:51.215853Z",
     "shell.execute_reply": "2024-04-14T14:09:51.215214Z"
    }
   },
   "outputs": [],
   "source": [
    "model.config.use_cache = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e29e91",
   "metadata": {},
   "source": [
    "> Start processing data\n",
    "\n",
    "> first is to apply template, we will then use `DataCollatorForCompletionOnlyLM` to mask non-assistant tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c8aa9384",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-14T14:09:51.218686Z",
     "iopub.status.busy": "2024-04-14T14:09:51.218187Z",
     "iopub.status.idle": "2024-04-14T14:09:51.443999Z",
     "shell.execute_reply": "2024-04-14T14:09:51.443129Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2-medium\")\n",
    "tokenizer.add_special_tokens({\"additional_special_tokens\": \\\n",
    "                              [\"<|system|>\",\"<|user|>\",\"<|assistant|>\",\"<|end|>\"]})\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "template_str = \"\"\"\\\n",
    "{% for message in messages %}\\\n",
    "{% if message[\"role\"] == \"user\" %}\\\n",
    "{{ \"<|endoftext|><|user|>\\n\" + message[\"content\"] + \"<|end|>\\n\" }}\\\n",
    "{% elif message[\"role\"] == \"system\" %}\\\n",
    "{{ \"<|system|>\\n\" + message[\"content\"] + \"<|end|>\\n\" }}\\\n",
    "{% elif message[\"role\"] == \"assistant\" %}\\\n",
    "{{ \"<|assistant|>\\n\" + message[\"content\"] + \"<|end|><|endoftext|>\\n\" }}\\\n",
    "{% endif %}\\\n",
    "{% endfor %}\\\n",
    "\"\"\"\n",
    "tokenizer.chat_template = template_str\n",
    "tokenizer.special_tokens_map\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# collator choice\n",
    "collator = DataCollatorForCompletionOnlyLM(\"<|assistant|>\", tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d7b74837",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-14T14:09:51.446930Z",
     "iopub.status.busy": "2024-04-14T14:09:51.446511Z",
     "iopub.status.idle": "2024-04-14T14:09:55.094622Z",
     "shell.execute_reply": "2024-04-14T14:09:55.093747Z"
    }
   },
   "outputs": [],
   "source": [
    "def pair_texts(examples):\n",
    "    # Concatenate all texts.\n",
    "    concatenated_examples = {key: list(itertools.chain(*value)) for key, value in examples.items()}\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "\n",
    "    result = {\n",
    "        k: [t[i : i + 2] for i in range(0, total_length, 2)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    return result\n",
    "\n",
    "def convert_messages(example):\n",
    "    t=[]\n",
    "    for ei in example[\"messages\"]:\n",
    "        if ei[\"role\"]==\"user\":\n",
    "            t.append(\"<|system|>Below is a dialogue between a human user and an AI assistant.<|end|>\\n\")\n",
    "            t.append(\"<|endoftext|><|user|>\" + ei[\"content\"] + \"<|end|>\\n\")\n",
    "        elif ei[\"role\"]==\"assistant\":\n",
    "            t.append(\"<|assistant|>\" + ei[\"content\"] + \"<|end|><|endoftext|>\")\n",
    "    example[\"messages\"]=\"\".join(t)\n",
    "    return example\n",
    "\n",
    "def truncation(example):\n",
    "    \n",
    "    _dialogue = tokenizer(example[\"messages\"],padding=\"max_length\",max_length=128,truncation=True)\n",
    "    \n",
    "    truncate_dialogue = tokenizer.decode(_dialogue[\"input_ids\"])\n",
    "    ending = \"original\"\n",
    "    if \"<|assistant|>\" in truncate_dialogue and \"<|end|><|endoftext|>\" not in truncate_dialogue:\n",
    "        _dialogue[\"input_ids\"][-2:] = [50259, 50256] # add <|end|><|endoftext|> -- cutoff assistant token\n",
    "        ending = \"assistant\"\n",
    "    if \"<|assistant|>\" not in truncate_dialogue:\n",
    "        _dialogue[\"input_ids\"][-2:] = [50259, 198]   # add <|end|>\\n -- no assistant token\n",
    "        ending = \"user\"\n",
    "    \n",
    "    example[\"ending\"]=ending\n",
    "    example[\"messages\"] = truncate_dialogue\n",
    "    return example\n",
    "\n",
    "def token_map(dataset):\n",
    "    dataset = dataset.map(pair_texts, batched=True)\n",
    "    dataset = dataset.map(convert_messages)\n",
    "    dataset = dataset.map(truncation)\n",
    "    dataset = dataset.filter(lambda example: example[\"ending\"]!=\"user\")\n",
    "    dataset = dataset.remove_columns([\"ending\"])\n",
    "    return dataset\n",
    "\n",
    "dataset = load_dataset(\"sablo/oasst2_curated\")\n",
    "dataset = token_map(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5ef4f6e4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-14T14:09:55.098131Z",
     "iopub.status.busy": "2024-04-14T14:09:55.097723Z",
     "iopub.status.idle": "2024-04-14T14:09:55.155107Z",
     "shell.execute_reply": "2024-04-14T14:09:55.154284Z"
    }
   },
   "outputs": [],
   "source": [
    "def tokenize_function(example):\n",
    "    outputs =  tokenizer(example[\"messages\"], \n",
    "                         padding=False, \n",
    "                         truncation=False,\n",
    "                         max_length=128,\n",
    "                         return_overflowing_tokens=False,\n",
    "                         return_length=False)\n",
    "    return {\"input_ids\": outputs[\"input_ids\"], \"attention_mask\": outputs[\"attention_mask\"]}\n",
    "train_dataset = dataset[\"train\"].map(tokenize_function, \n",
    "                                     batched=True, \n",
    "                                     remove_columns=dataset[\"train\"].column_names,\n",
    "                                     num_proc=2,\n",
    "                                     batch_size=1000)\n",
    "test_dataset = dataset[\"test\"].map(tokenize_function, \n",
    "                                   batched=True, \n",
    "                                   remove_columns=dataset[\"test\"].column_names,\n",
    "                                   num_proc=2,\n",
    "                                   batch_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "08394ba9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-14T14:09:55.158293Z",
     "iopub.status.busy": "2024-04-14T14:09:55.157641Z",
     "iopub.status.idle": "2024-04-14T14:09:55.161985Z",
     "shell.execute_reply": "2024-04-14T14:09:55.161196Z"
    }
   },
   "outputs": [],
   "source": [
    "# for training (i.e., fine-tuning)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True, collate_fn=collator, pin_memory=True)\n",
    "# for getting testing perplexity\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=64, shuffle=False, collate_fn=collator, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf2d7df4",
   "metadata": {},
   "source": [
    "> metric function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "205dd815",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-14T14:09:55.164619Z",
     "iopub.status.busy": "2024-04-14T14:09:55.164203Z",
     "iopub.status.idle": "2024-04-14T14:09:55.168885Z",
     "shell.execute_reply": "2024-04-14T14:09:55.168129Z"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate(dataloader):\n",
    "    model.eval()\n",
    "    loss = 0\n",
    "    for batch in dataloader:\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**batch)\n",
    "        loss += outputs.loss.item() * batch[\"input_ids\"].shape[0]\n",
    "    loss = loss / len(train_dataloader.dataset)\n",
    "    try:\n",
    "        ppl = np.exp(loss)\n",
    "    except OverflowError:\n",
    "        ppl = float(\"inf\")\n",
    "    return loss, ppl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffe4c147",
   "metadata": {},
   "source": [
    "> define optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5b3ef635",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-14T14:09:55.171587Z",
     "iopub.status.busy": "2024-04-14T14:09:55.171262Z",
     "iopub.status.idle": "2024-04-14T14:09:55.176228Z",
     "shell.execute_reply": "2024-04-14T14:09:55.175469Z"
    }
   },
   "outputs": [],
   "source": [
    "# see paper https://arxiv.org/abs/2006.00719\n",
    "# with limited search, we see that\n",
    "# 1e-1 is the best learning rate \n",
    "optimizer = optim.Adahessian(model.parameters(), lr=1e-1) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c402ec",
   "metadata": {},
   "source": [
    "> training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c708e80e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-14T14:09:55.178906Z",
     "iopub.status.busy": "2024-04-14T14:09:55.178627Z",
     "iopub.status.idle": "2024-04-14T14:09:55.181892Z",
     "shell.execute_reply": "2024-04-14T14:09:55.181215Z"
    }
   },
   "outputs": [],
   "source": [
    "# log stats\n",
    "# we can measure computational cost by simply logging gradient^ computations\n",
    "#\n",
    "# ^ for second-order methods, in particular, Hessian-free methods, we use gradient and Hessian-vector\n",
    "# product computation as a measure of cost\n",
    "logger = [] # ep, it, loss, ppl, gradient/Hv product, mem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "45b63363",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-14T14:09:55.184612Z",
     "iopub.status.busy": "2024-04-14T14:09:55.184046Z",
     "iopub.status.idle": "2024-04-14T16:21:21.842057Z",
     "shell.execute_reply": "2024-04-14T16:21:21.841416Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08c73b15601e41ab85aaa46fb4089f1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/441 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, iter: 0, tr_loss: 9.05e+01, tr_ppl: 2.10e+39, cost: 0, mem: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/llm/optim/.venv/lib/python3.10/site-packages/torch/autograd/__init__.py:266: UserWarning: Using backward() with create_graph=True will create a reference cycle between the parameter and its gradient which can cause a memory leak. We recommend using autograd.grad when creating the graph to avoid this. If you have to use this function, make sure to reset the .grad fields of your parameters to None after use to break the cycle and avoid the leak. (Triggered internally at ../torch/csrc/autograd/engine.cpp:1177.)\n",
      "  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, iter: 10, tr_loss: 7.78e+00, tr_ppl: 1.52e+00, cost: 20, mem: 9.83e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, iter: 20, tr_loss: 7.64e+00, tr_ppl: 1.51e+00, cost: 40, mem: 1.29e+01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, iter: 30, tr_loss: 7.55e+00, tr_ppl: 1.51e+00, cost: 60, mem: 1.29e+01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, iter: 40, tr_loss: 7.45e+00, tr_ppl: 1.50e+00, cost: 80, mem: 1.29e+01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, iter: 50, tr_loss: 7.33e+00, tr_ppl: 1.49e+00, cost: 100, mem: 1.29e+01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, iter: 60, tr_loss: 7.32e+00, tr_ppl: 1.48e+00, cost: 120, mem: 1.29e+01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, iter: 70, tr_loss: 7.51e+00, tr_ppl: 1.50e+00, cost: 140, mem: 1.29e+01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, iter: 80, tr_loss: 7.51e+00, tr_ppl: 1.50e+00, cost: 160, mem: 1.29e+01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, iter: 90, tr_loss: 7.47e+00, tr_ppl: 1.50e+00, cost: 180, mem: 1.29e+01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, iter: 100, tr_loss: 7.42e+00, tr_ppl: 1.50e+00, cost: 200, mem: 1.29e+01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, iter: 110, tr_loss: 7.38e+00, tr_ppl: 1.49e+00, cost: 220, mem: 1.29e+01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, iter: 120, tr_loss: 7.33e+00, tr_ppl: 1.49e+00, cost: 240, mem: 1.29e+01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, iter: 130, tr_loss: 7.24e+00, tr_ppl: 1.48e+00, cost: 260, mem: 1.29e+01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, iter: 140, tr_loss: 7.12e+00, tr_ppl: 1.47e+00, cost: 280, mem: 1.29e+01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, iter: 150, tr_loss: 6.87e+00, tr_ppl: 1.45e+00, cost: 300, mem: 1.29e+01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, iter: 160, tr_loss: 6.36e+00, tr_ppl: 1.41e+00, cost: 320, mem: 1.29e+01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, iter: 170, tr_loss: 1.23e+01, tr_ppl: 1.93e+00, cost: 340, mem: 1.29e+01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, iter: 180, tr_loss: 7.84e+00, tr_ppl: 1.53e+00, cost: 360, mem: 1.29e+01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, iter: 190, tr_loss: 7.61e+00, tr_ppl: 1.51e+00, cost: 380, mem: 1.29e+01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, iter: 200, tr_loss: 7.54e+00, tr_ppl: 1.51e+00, cost: 400, mem: 1.29e+01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, iter: 210, tr_loss: 7.53e+00, tr_ppl: 1.50e+00, cost: 420, mem: 1.29e+01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, iter: 220, tr_loss: 7.51e+00, tr_ppl: 1.50e+00, cost: 440, mem: 1.29e+01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, iter: 230, tr_loss: 7.50e+00, tr_ppl: 1.50e+00, cost: 460, mem: 1.29e+01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, iter: 240, tr_loss: 7.49e+00, tr_ppl: 1.50e+00, cost: 480, mem: 1.29e+01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, iter: 250, tr_loss: 7.48e+00, tr_ppl: 1.50e+00, cost: 500, mem: 1.29e+01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, iter: 260, tr_loss: 7.48e+00, tr_ppl: 1.50e+00, cost: 520, mem: 1.29e+01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, iter: 270, tr_loss: 7.47e+00, tr_ppl: 1.50e+00, cost: 540, mem: 1.29e+01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, iter: 280, tr_loss: 7.46e+00, tr_ppl: 1.50e+00, cost: 560, mem: 1.29e+01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, iter: 290, tr_loss: 7.46e+00, tr_ppl: 1.50e+00, cost: 580, mem: 1.29e+01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, iter: 300, tr_loss: 7.45e+00, tr_ppl: 1.50e+00, cost: 600, mem: 1.29e+01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, iter: 310, tr_loss: 7.45e+00, tr_ppl: 1.50e+00, cost: 620, mem: 1.29e+01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, iter: 320, tr_loss: 7.45e+00, tr_ppl: 1.50e+00, cost: 640, mem: 1.29e+01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, iter: 330, tr_loss: 7.44e+00, tr_ppl: 1.50e+00, cost: 660, mem: 1.29e+01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, iter: 340, tr_loss: 7.44e+00, tr_ppl: 1.50e+00, cost: 680, mem: 1.29e+01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, iter: 350, tr_loss: 7.43e+00, tr_ppl: 1.50e+00, cost: 700, mem: 1.29e+01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, iter: 360, tr_loss: 7.43e+00, tr_ppl: 1.50e+00, cost: 720, mem: 1.29e+01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, iter: 370, tr_loss: 7.43e+00, tr_ppl: 1.50e+00, cost: 740, mem: 1.29e+01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, iter: 380, tr_loss: 7.42e+00, tr_ppl: 1.50e+00, cost: 760, mem: 1.29e+01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, iter: 390, tr_loss: 7.42e+00, tr_ppl: 1.50e+00, cost: 780, mem: 1.29e+01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, iter: 400, tr_loss: 7.42e+00, tr_ppl: 1.50e+00, cost: 800, mem: 1.29e+01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, iter: 410, tr_loss: 7.42e+00, tr_ppl: 1.50e+00, cost: 820, mem: 1.29e+01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, iter: 420, tr_loss: 7.41e+00, tr_ppl: 1.49e+00, cost: 840, mem: 1.29e+01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, iter: 430, tr_loss: 7.41e+00, tr_ppl: 1.49e+00, cost: 860, mem: 1.29e+01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, iter: 440, tr_loss: 7.41e+00, tr_ppl: 1.49e+00, cost: 880, mem: 1.29e+01\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 1\n",
    "num_training_steps = num_epochs * len(train_dataloader)\n",
    "# start memory\n",
    "start_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "# progress bar\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "# cost\n",
    "cost = 0\n",
    "# training loop\n",
    "for epoch in range(num_epochs):\n",
    "    # initialize memory stats\n",
    "    used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3) - start_memory\n",
    "    # compute (training) batch loss and (testing) perplexity\n",
    "    batch_loss, batch_ppl = evaluate(train_dataloader)\n",
    "    # for this demo, we really don't care about validation or testing\n",
    "    # please uncomment the following line if needed\n",
    "    # _, batch_ppl = evaluate(test_dataloader)\n",
    "    # logger\n",
    "    logger.append((epoch, 0, batch_loss, batch_ppl, cost, used_memory))\n",
    "    # print out results\n",
    "    print(f\"epoch: {epoch}, iter: {0}, tr_loss: {batch_loss:.2e}, tr_ppl: {batch_ppl:.2e}, cost: {cost}, mem: {used_memory}\")\n",
    "    model.train()\n",
    "    step_cnt = 0\n",
    "    for it, minibatch in enumerate(train_dataloader, 1):\n",
    "        # forward pass\n",
    "        outputs = model(**minibatch)\n",
    "        loss = outputs.loss\n",
    "        # backward pass - `create_graph=True` is neccessary for AdaHessian\n",
    "        loss.backward(create_graph=True)\n",
    "        # optimization step\n",
    "        optimizer.step()\n",
    "        # cost\n",
    "        cost += 2 # each step requires two Gradient/Hv-product computations\n",
    "        optimizer.zero_grad()\n",
    "        # update progress bar\n",
    "        progress_bar.update(1)\n",
    "        \n",
    "        if it % 10 == 0:\n",
    "            # re-compute used memory\n",
    "            used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3) - start_memory\n",
    "            # compute (training) batch loss and (testing) perplexity\n",
    "            batch_loss, batch_ppl = evaluate(train_dataloader)\n",
    "            _, batch_ppl = evaluate(test_dataloader)\n",
    "             # logger\n",
    "            logger.append((epoch, it, batch_loss, batch_ppl, used_memory))\n",
    "            # print out results\n",
    "            print(f\"epoch: {epoch}, iter: {it}, tr_loss: {batch_loss:.2e}, tr_ppl: {batch_ppl:.2e}, cost: {cost}, mem: {used_memory:.2e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e12b9351",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-14T16:21:21.844617Z",
     "iopub.status.busy": "2024-04-14T16:21:21.844261Z",
     "iopub.status.idle": "2024-04-14T16:21:21.848351Z",
     "shell.execute_reply": "2024-04-14T16:21:21.847772Z"
    }
   },
   "outputs": [],
   "source": [
    "with open('adahessian_results.pickle', 'wb') as f:\n",
    "    pickle.dump(logger, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1a614100",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-14T16:21:21.850952Z",
     "iopub.status.busy": "2024-04-14T16:21:21.850463Z",
     "iopub.status.idle": "2024-04-14T16:21:21.853629Z",
     "shell.execute_reply": "2024-04-14T16:21:21.853132Z"
    }
   },
   "outputs": [],
   "source": [
    "# Exit the program with a success status\n",
    "# also release memory on GPU\n",
    "exit(0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "optimizer_lora",
   "language": "python",
   "name": "optimizerlora"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "08c73b15601e41ab85aaa46fb4089f1b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_5055828e0c3748b0a88687079e993d34",
        "IPY_MODEL_15a4e004e918430fbc6f7ea4e7b3bbaa",
        "IPY_MODEL_3ee4fe4cc83c4d979f408505c02d2405"
       ],
       "layout": "IPY_MODEL_b9e7ecf8072c4701a9022d81c101ed39",
       "tabbable": null,
       "tooltip": null
      }
     },
     "0beea12c1c794494a0db6dad48222812": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "15a4e004e918430fbc6f7ea4e7b3bbaa": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_8a85965da2a449798f55132039510291",
       "max": 441.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_5f05ea3232224b6d8e4c58b3983ffe87",
       "tabbable": null,
       "tooltip": null,
       "value": 441.0
      }
     },
     "3ee4fe4cc83c4d979f408505c02d2405": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_0beea12c1c794494a0db6dad48222812",
       "placeholder": "​",
       "style": "IPY_MODEL_b63323b5e1c74416967f9be834c8f7b0",
       "tabbable": null,
       "tooltip": null,
       "value": " 441/441 [2:11:26&lt;00:00, 52.07s/it]"
      }
     },
     "5055828e0c3748b0a88687079e993d34": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_e482cfb9a7b542aaabc5ce94f9d22d57",
       "placeholder": "​",
       "style": "IPY_MODEL_d3a37c602746498691bbddd30ef36b44",
       "tabbable": null,
       "tooltip": null,
       "value": "100%"
      }
     },
     "5f05ea3232224b6d8e4c58b3983ffe87": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "8a85965da2a449798f55132039510291": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "b63323b5e1c74416967f9be834c8f7b0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "b9e7ecf8072c4701a9022d81c101ed39": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "d3a37c602746498691bbddd30ef36b44": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "e482cfb9a7b542aaabc5ce94f9d22d57": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
