{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2eaf0af",
   "metadata": {},
   "outputs": [],
   "source": [
    "! nvidia-smi | grep -B 0 \"W\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23cd9101",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "659151a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import itertools\n",
    "from torch import nn\n",
    "from trl import DataCollatorForCompletionOnlyLM\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "import bitsandbytes as bnb\n",
    "from torch.optim import lr_scheduler\n",
    "from functools import partial\n",
    "from transformers import get_scheduler\n",
    "from transformers.trainer_pt_utils import get_parameter_names\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, TrainingArguments, get_scheduler, AutoModelForCausalLM,\\\n",
    "DataCollatorWithPadding, DataCollatorForLanguageModeling, BitsAndBytesConfig\n",
    "from tqdm.auto import tqdm\n",
    "from accelerate import Accelerator\n",
    "from peft import LoraConfig, TaskType, get_peft_model, PeftModelForCausalLM, prepare_model_for_kbit_training\n",
    "from datasets import load_dataset\n",
    "import sys\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c731e1",
   "metadata": {},
   "source": [
    "> set random seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b355a340",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7acfca3e",
   "metadata": {},
   "source": [
    "> we load model in 4bit, which is actually stored as `torch.uint8` and \n",
    ">\n",
    "> we compute in fp16\n",
    ">\n",
    "> Note that, with `prepare_model_for_kbit_training`, it is actually fp32\n",
    "> for AdaHessian and TRCG to work, fp32 is required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e6984b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=\"float16\",\n",
    ")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"openai-community/gpt2\",\n",
    "                                             torch_dtype=torch.float16,\n",
    "                                             device_map=\"auto\", \n",
    "                                             quantization_config=config)\n",
    "model = prepare_model_for_kbit_training(model, use_gradient_checkpointing=False )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9f127c7",
   "metadata": {},
   "source": [
    "> prep for LoRA model\n",
    ">\n",
    "> check http://localhost:1331/edit/optim/notebooks/README.md\n",
    "> \n",
    "> for currently implemented default `target modules` in `peft`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "608abb3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=4,\n",
    "    lora_alpha=32, # based on paper - https://arxiv.org/abs/2106.09685\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    lora_dropout=0.05\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5c90d6d",
   "metadata": {},
   "source": [
    "> at this moment, we need to disable `gradient checkpoint` for\n",
    "> current implementation of AdaHessian and TRCG to work\n",
    "> \n",
    "> Here, for fair comparison, turn off GC for AdamW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b570d99b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.is_gradient_checkpointing, model.quantization_method, model.dtype,\\\n",
    "model.is_loaded_in_4bit,\\\n",
    "model.config.use_cache"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e342e612",
   "metadata": {},
   "source": [
    "> set `use_cache` to False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4736e22c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.use_cache = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d79ce2f",
   "metadata": {},
   "source": [
    "> Start processing data\n",
    "\n",
    "> first is to apply template, we will then use `DataCollatorForCompletionOnlyLM` to mask non-assistant tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8aa9384",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2-medium\")\n",
    "tokenizer.add_special_tokens({\"additional_special_tokens\": \\\n",
    "                              [\"<|system|>\",\"<|user|>\",\"<|assistant|>\",\"<|end|>\"]})\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "template_str = \"\"\"\\\n",
    "{% for message in messages %}\\\n",
    "{% if message[\"role\"] == \"user\" %}\\\n",
    "{{ \"<|endoftext|><|user|>\\n\" + message[\"content\"] + \"<|end|>\\n\" }}\\\n",
    "{% elif message[\"role\"] == \"system\" %}\\\n",
    "{{ \"<|system|>\\n\" + message[\"content\"] + \"<|end|>\\n\" }}\\\n",
    "{% elif message[\"role\"] == \"assistant\" %}\\\n",
    "{{ \"<|assistant|>\\n\" + message[\"content\"] + \"<|end|><|endoftext|>\\n\" }}\\\n",
    "{% endif %}\\\n",
    "{% endfor %}\\\n",
    "\"\"\"\n",
    "tokenizer.chat_template = template_str\n",
    "tokenizer.special_tokens_map\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# collator choice\n",
    "collator = DataCollatorForCompletionOnlyLM(\"<|assistant|>\", tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b74837",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pair_texts(examples):\n",
    "    # Concatenate all texts.\n",
    "    concatenated_examples = {key: list(itertools.chain(*value)) for key, value in examples.items()}\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "\n",
    "    result = {\n",
    "        k: [t[i : i + 2] for i in range(0, total_length, 2)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    return result\n",
    "\n",
    "def convert_messages(example):\n",
    "    t=[]\n",
    "    for ei in example[\"messages\"]:\n",
    "        if ei[\"role\"]==\"user\":\n",
    "            t.append(\"<|system|>Below is a dialogue between a human user and an AI assistant.<|end|>\\n\")\n",
    "            t.append(\"<|endoftext|><|user|>\" + ei[\"content\"] + \"<|end|>\\n\")\n",
    "        elif ei[\"role\"]==\"assistant\":\n",
    "            t.append(\"<|assistant|>\" + ei[\"content\"] + \"<|end|><|endoftext|>\")\n",
    "    example[\"messages\"]=\"\".join(t)\n",
    "    return example\n",
    "\n",
    "def truncation(example):\n",
    "    \n",
    "    _dialogue = tokenizer(example[\"messages\"],padding=\"max_length\",max_length=128,truncation=True)\n",
    "    \n",
    "    truncate_dialogue = tokenizer.decode(_dialogue[\"input_ids\"])\n",
    "    ending = \"original\"\n",
    "    if \"<|assistant|>\" in truncate_dialogue and \"<|end|><|endoftext|>\" not in truncate_dialogue:\n",
    "        _dialogue[\"input_ids\"][-2:] = [50259, 50256] # add <|end|><|endoftext|> -- cutoff assistant token\n",
    "        ending = \"assistant\"\n",
    "    if \"<|assistant|>\" not in truncate_dialogue:\n",
    "        _dialogue[\"input_ids\"][-2:] = [50259, 198]   # add <|end|>\\n -- no assistant token\n",
    "        ending = \"user\"\n",
    "    \n",
    "    example[\"ending\"]=ending\n",
    "    example[\"messages\"] = truncate_dialogue\n",
    "    return example\n",
    "\n",
    "def token_map(dataset):\n",
    "    dataset = dataset.map(pair_texts, batched=True)\n",
    "    dataset = dataset.map(convert_messages)\n",
    "    dataset = dataset.map(truncation)\n",
    "    dataset = dataset.filter(lambda example: example[\"ending\"]!=\"user\")\n",
    "    dataset = dataset.remove_columns([\"ending\"])\n",
    "    return dataset\n",
    "\n",
    "dataset = load_dataset(\"sablo/oasst2_curated\")\n",
    "dataset = token_map(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef4f6e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(example):\n",
    "    outputs =  tokenizer(example[\"messages\"], \n",
    "                         padding=False, \n",
    "                         truncation=False,\n",
    "                         max_length=128,\n",
    "                         return_overflowing_tokens=False,\n",
    "                         return_length=False)\n",
    "    return {\"input_ids\": outputs[\"input_ids\"], \"attention_mask\": outputs[\"attention_mask\"]}\n",
    "train_dataset = dataset[\"train\"].map(tokenize_function, \n",
    "                                     batched=True, \n",
    "                                     remove_columns=dataset[\"train\"].column_names,\n",
    "                                     num_proc=2,\n",
    "                                     batch_size=1000)\n",
    "test_dataset = dataset[\"test\"].map(tokenize_function, \n",
    "                                   batched=True, \n",
    "                                   remove_columns=dataset[\"test\"].column_names,\n",
    "                                   num_proc=2,\n",
    "                                   batch_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08394ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for training (i.e., fine-tuning)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True, collate_fn=collator, pin_memory=True)\n",
    "# for getting testing perplexity\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=64, shuffle=False, collate_fn=collator, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fce52e1",
   "metadata": {},
   "source": [
    "> AdamW\n",
    "\n",
    "> we need to define parameter group\n",
    "> this is due to the fact that we will\n",
    "> only apply weight decay on non-bias params\n",
    "\n",
    "> we also define algorithm args here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9903af27",
   "metadata": {},
   "outputs": [],
   "source": [
    "decay_parameters = get_parameter_names(model, [nn.LayerNorm])\n",
    "decay_parameters = [name for name in decay_parameters if \"bias\" not in name]\n",
    "\n",
    "learning_rate = 5e-4 # so far the best choice (note: w/o exhaustive search)\n",
    "beta1, beta2 = 0.9, 0.999 # most of time, no need to change\n",
    "epsilon = 1e-8\n",
    "weight_decay = 1e-2 # so far the best w/o exhaustive search\n",
    "\n",
    "optimizer_kwargs = {\n",
    "    \"lr\": learning_rate,\n",
    "    \"betas\": (beta1, beta2),\n",
    "    \"eps\": epsilon,\n",
    "}\n",
    "\n",
    "optimizer_grouped_parameters = [\n",
    "        {\n",
    "            \"params\": [\n",
    "                p for n, p in model.named_parameters() if (n in decay_parameters and p.requires_grad)\n",
    "            ],\n",
    "            \"weight_decay\": weight_decay,\n",
    "        },\n",
    "        {\n",
    "            \"params\": [\n",
    "                p for n, p in model.named_parameters() if (n not in decay_parameters and p.requires_grad)\n",
    "            ],\n",
    "            \"weight_decay\": 0.0,\n",
    "        },\n",
    "    ]\n",
    "\n",
    "optimizer = torch.optim.AdamW(optimizer_grouped_parameters, **optimizer_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f24f0c",
   "metadata": {},
   "source": [
    "> define scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33bd0603",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 1\n",
    "num_training_steps = num_epochs * len(train_dataloader)\n",
    "num_warmup_steps = 10 # could be increased\n",
    "name = \"linear\"\n",
    "scheduler = get_scheduler(name=name,\n",
    "                          optimizer=optimizer,\n",
    "                          num_warmup_steps=num_warmup_steps,\n",
    "                          num_training_steps=num_training_steps\n",
    "                         )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec55aa7d",
   "metadata": {},
   "source": [
    "> metric function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc6b3f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(dataloader):\n",
    "    model.eval()\n",
    "    loss = 0\n",
    "    for batch in dataloader:\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**batch)\n",
    "        loss += outputs.loss.item() * batch[\"input_ids\"].shape[0]\n",
    "    loss = loss / len(train_dataloader.dataset)\n",
    "    try:\n",
    "        ppl = np.exp(loss)\n",
    "    except OverflowError:\n",
    "        ppl = float(\"inf\")\n",
    "    return loss, ppl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8620e01",
   "metadata": {},
   "source": [
    "> training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be0f0a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# log stats\n",
    "logger = [] # ep, it, loss, ppl, mem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "265788dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 1\n",
    "num_training_steps = num_epochs * len(train_dataloader)\n",
    "# start memory\n",
    "start_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "# progress bar\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "for epoch in range(num_epochs):\n",
    "    # initialize memory stats\n",
    "    used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3) - start_memory\n",
    "    # compute (training) batch loss and (testing) perplexity\n",
    "    batch_loss, batch_ppl = evaluate(train_dataloader)\n",
    "#     _, batch_ppl = evaluate(test_dataloader)\n",
    "    # logger\n",
    "    logger.append((epoch, 0, batch_loss, batch_ppl, used_memory))\n",
    "    # print out results\n",
    "    print(f\"epoch: {epoch}, iter: {0}, tr_loss: {batch_loss:.2e}, te_ppl: {batch_ppl:.2e}, mem: {used_memory}\")\n",
    "    model.train()\n",
    "    step_cnt = 0\n",
    "    for it, minibatch in enumerate(train_dataloader, 1):\n",
    "        # forward pass\n",
    "        outputs = model(**minibatch)\n",
    "        loss = outputs.loss\n",
    "        # backward pass\n",
    "        loss.backward()\n",
    "        # optimization step\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        # update progress bar\n",
    "        progress_bar.update(1)\n",
    "        \n",
    "        if it % 10 == 0:\n",
    "            # re-compute used memory\n",
    "            used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3) - start_memory\n",
    "            # compute (training) batch loss and (testing) perplexity\n",
    "            batch_loss, batch_ppl = evaluate(train_dataloader)\n",
    "#             _, batch_ppl = evaluate(test_dataloader)\n",
    "             # logger\n",
    "            logger.append((epoch, it, batch_loss, batch_ppl, used_memory))\n",
    "            # print out results\n",
    "            print(f\"epoch: {epoch}, iter: {it}, tr_loss: {batch_loss:.2e}, te_ppl: {batch_ppl:.2e}, mem: {used_memory:.2e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc037d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exit the program with a success status\n",
    "# also release memory on GPU\n",
    "exit(0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "optimizer_lora",
   "language": "python",
   "name": "optimizerlora"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
